# Use a slim Python base image
FROM python:3.12-slim

# Install required system packages for building llama-cpp-python
RUN apt-get update && apt-get install -y --no-install-recommends \
        build-essential \
        cmake \
        git \
        libopenblas-dev \
        pkg-config \
    && rm -rf /var/lib/apt/lists/*

# Enable OpenBLAS acceleration for llama.cpp
ENV CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS"

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip \
    && pip install --no-cache-dir \
        fastapi \
        uvicorn \
        llama-cpp-python \
        huggingface-hub

# Create directory for model
RUN mkdir -p /models

# Download the StarCoder2 3B 4-bit quantized model during build
RUN python - <<'EOF'
from huggingface_hub import hf_hub_download
path = hf_hub_download(
    repo_id="second-state/StarCoder2-3B-GGUF",
    filename="starcoder2-3b-Q4_K_S.gguf",
    local_dir="/models",
    local_dir_use_symlinks=False
)
print("Model downloaded to:", path)
EOF

WORKDIR /app

# Copy API code
COPY main.py .

# Expose FastAPI port
EXPOSE 8000

# Start server
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]